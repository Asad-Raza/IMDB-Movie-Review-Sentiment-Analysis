{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM IMPLEMENTATION KERAS EXPLANATION\n",
    "\n",
    "\n",
    "TO BEGIN ALSO SEE A DETAILED EXPLANATION OF KERAS EMBEDDING LAYER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPORTING MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OlvqDedErrB3",
    "outputId": "0019ea59-6cea-49e0-c6c8-872f4f73a635"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Ignore  the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# data visualisation and manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "#configure\n",
    "# sets matplotlib to inline and displays graphs below the corressponding cell.\n",
    "% matplotlib inline  \n",
    "style.use('fivethirtyeight')\n",
    "sns.set(style='whitegrid',color_codes=True)\n",
    "\n",
    "#import nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# BeautifulSoup libraray\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "import re # regex\n",
    "\n",
    "#keras\n",
    "import keras\n",
    "from keras.preprocessing.text import one_hot,Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Flatten ,Embedding,Input,Dropout,LSTM,CuDNNLSTM\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "#model selection\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREATING A SAMPLE CORPUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I am creating a sampple corpus of just 3 documents which should be enough to depict the working of keras embedding layer\n",
    "as well as the implementation of lstm in Keras librray. \n",
    "\n",
    "**Do not focus on the accurcay while training as the dtaa is just purely random(bas aise hi kuch bhi utha liya hai ki chal jaye main purpose lstm ko implement karna hai)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lqvcPDLoFjFk"
   },
   "outputs": [],
   "source": [
    "sample_text_1=\"bitty bought a bit of butter\"\n",
    "sample_text_2=\"but the bit of butter was a bit bitter\"\n",
    "sample_text_3=\"so she bought some better butter to make the bitter butter better\"\n",
    "\n",
    "corp=[sample_text_1,sample_text_2,sample_text_3]\n",
    "no_docs=len(corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INTEGER ENCODING THE DOCUMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "qnwAC4t0HVvc",
    "outputId": "47aa1be5-5ce0-414a-b6eb-f3794bc7177e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The encoding for document 1  is :  [5, 14, 17, 6, 35, 18]\n",
      "The encoding for document 2  is :  [46, 48, 6, 35, 18, 6, 17, 6, 47]\n",
      "The encoding for document 3  is :  [49, 20, 14, 17, 33, 18, 28, 2, 48, 47, 18, 33]\n"
     ]
    }
   ],
   "source": [
    "vocab_size=50 \n",
    "encod_corp=[]\n",
    "for i,doc in enumerate(corp):\n",
    "    encod_corp.append(one_hot(doc,50))\n",
    "    print(\"The encoding for document\",i+1,\" is : \",one_hot(doc,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "D6FuXYHRHeqj",
    "outputId": "c57bc773-7ebc-4807-d088-c71b3c4f9feb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum number of words in any document is :  12\n"
     ]
    }
   ],
   "source": [
    "maxlen=-1\n",
    "for doc in corp:\n",
    "    tokens=nltk.word_tokenize(doc)\n",
    "    if(maxlen<len(tokens)):\n",
    "        maxlen=len(tokens)\n",
    "print(\"The maximum number of words in any document is : \",maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PADDING THE DOCUMENTS SO THAT ALL ARE OF SAME LENGTH AS REQD BY KERAS EMBEDDING LAYER\n",
    "\n",
    "**Note that there is one way to work with variable lenght reviews in keraslike b suing some 'Masking' layer or by specifying 'NOne' in place of 'max_doc_len' in 'input_lenght' in Keras or in 'Input(shape=(None,))' when using function api of keras.\n",
    "but ye tarike ko sai se karna kaise hai that is not very clear to me at the moment.**\n",
    "\n",
    "**Also one approach widely mentioned on the net is that different batches while training (batch_size that we specify) can have different lengths although obervations (or text as dealing with text classification or reviews in sentiment analysis) within same batch should have same size.(like all reviews in same batch should have same no of words). so what we can do is specify batch_size while training equal to 1 but that is too inefficient.**\n",
    "\n",
    "**to improve upon batch_size of 1 what we can do is to group observations with same text in same batches (like say for sentiment analysis ke liye reviews ko no of words ke hissab se sort kar loo and then jinme same no of woeds ho unkko same batchesme daal doo). bUt ye traika thoda ajeeb lagra hai and isliye abhi nai kiya hai though [iss](https://adventuresinmachinelearning.com/keras-lstm-tutorial/) blog me SHAYAD aisa hi kuch kiy ahai.**\n",
    "\n",
    "\n",
    "**Last option is to pad the reviews or texts and to make all of them to be of same length.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Y1XVioZAHjCB",
    "outputId": "2e4a0b39-0f7e-43d9-b849-a66a9f7801d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of padded documents:  3\n"
     ]
    }
   ],
   "source": [
    "# now to create embeddings all of our docs need to be of same length. hence we can pad the docs with zeros.\n",
    "pad_corp=pad_sequences(encod_corp,maxlen=maxlen,padding='post',value=0.0)\n",
    "print(\"No of padded documents: \",len(pad_corp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "9miuseoPHoCO",
    "outputId": "06e96c53-f546-4a28-a404-caecdaedf5e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The padded encoding for document 1  is :  [ 5 14 17  6 35 18  0  0  0  0  0  0]\n",
      "The padded encoding for document 2  is :  [46 48  6 35 18  6 17  6 47  0  0  0]\n",
      "The padded encoding for document 3  is :  [49 20 14 17 33 18 28  2 48 47 18 33]\n"
     ]
    }
   ],
   "source": [
    "for i,doc in enumerate(pad_corp):\n",
    "     print(\"The padded encoding for document\",i+1,\" is : \",doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kmBRhR5_ITMi"
   },
   "outputs": [],
   "source": [
    "vocab_size=vocab_size\n",
    "embed_size=100\n",
    "max_doc_len=maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREATING AND BUILDING THE EMBEDDINGS AND TEHN LSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wKUPPo_hGp1J"
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size,output_dim=embed_size,input_length=max_doc_len))\n",
    "model.add(CuDNNLSTM(64,return_sequences=False))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BREAKING IT DOWN ---\n",
    "\n",
    "1) sabse pehle hum apne text ya reviews ki embeddings banayenge. refer keras ki embedding layer ki explanation agar koi doubt hora hai. is layer ka output jo hai uski shape jo hai vo (no of samples ,padded lenght, embedding size hoga).  for eg is sentiment analysis ke case me embedding layer ka output jo hai uska shape (no of reviews,no of words,embedding size) hoga.\n",
    "\n",
    "2) ab next step hai is embedding ko lstm me sai tareeka se feed karna. First of all normal LSTM layer keras ki even Kaggle or Google cloud ke gpu pe bhut zyada slow hai isliye use CuDNNLSTM jo ki khali gpu ke liye hai.\n",
    "\n",
    "**ab aati hai baari lstm ki layers ke input aur output or parametsr ki :**\n",
    "\n",
    "**A) units : ** units jo hai vo output jo niklega lstm se uska dimension hai. actually agar week 1 of NG ke dl wale course se padoge to ye lstm la c(t) ya a(t) ka size hai. (size or number of hidden states or memory cell state).kai jagah like famous Colah ke blog me isse h(t) se denote kiya hai. for eg agar h(t) ek vector hhai to us vector ka size ya dimensions ye unit parameter hai. nutshell jo har timestep pe cell state(c(t)) ya output(h(t)) hai uska dimension hai ye.\n",
    "\n",
    "**NOTE THAT YE NO OF LSTM CELLS NAI HAI LIKE JAISE COLAH KE DIAGRAM ME JO CELLS BANE HAI YTA JO NG BANAT HAI YE VO NO OF CELLS NAI HAI BALKI OUTPUT DIMENSION HAI.**\n",
    "\n",
    "**FOR BETTER INITUITION SEE MODEL SUMMARY.**\n",
    "\n",
    "\n",
    "**B )return_sequences : ** ye jo hai keval tab chhaiye hota hai jab lstm layers stack karni hai ya lstm +gru. basically agar say har lstm cell se output chahiye like in machine translation (jaha jar cell ka output ekword hoga) ya in general jaha bhi output bhi ek sequence hai vaha chahiye hoga. kyunki yaha hume koi sequcne nai bas + ya _ chahiye isliye yaha iski zaroorat nai hai isliye false.\n",
    "\n",
    "**AGAR YE FALSE HAI TO YE many-to-one lstm n/w hoga varna many-to-many.** MUST READ ANSWER ON SO **[this](https://stackoverflow.com/questions/38714959/understanding-keras-lstms)**.\n",
    "\n",
    "**C ) JUST TO MENTION KUCH BLOGS PE input_shape BHI PARAMETER DIYA HOTA HAI lstm layer me jo mujhe to doc me dikhaaye nai diya. ig vo tab chahiye hoga jab input like x_train vagerah khud hi shape me hai and pehle embedding layer nai use karre hai. AND ISI ME AGAR input_shape=(None,) likh de to vo variable reviews ko bhi handle kar sakte hai but again ye cheez mujhe zyada clr nai hui**\n",
    "\n",
    "**D )ITNA SAB LIKHNE KE BAAD BHI EK CHEEZ BACHHI VO HAI KI HUMNE NO OF TIMESTEPS YA NO OF WORDS (HAR TIME STEP PE EK WORD SEE NG SENTIMENT CLASSIFICATION VIDEO) VO KAHA SPECIFY KIYA HAI. TO JITNI MERI UNDERSTANDING HAI VO YE KEHTI HAI KI VO EMBEDDING LAYER KE OUTPUT SE HI UNDERSTOOD HAI. LIKE NO OF TIME-STEPS JO HAI VO NO OF WORDS HONGE AND HAR INPUT KA SIZE JO HAI (x(t)) in NG VO EMBEDDING KA SIZE YAANI HAR WORD KO JITNE ME EMBED KIYA AI USKASIZE HO JAAYEGA. THEREFORE RQED INPUT FOR LSTM IS **\n",
    "\n",
    "**(samples,timesteps,features) which boils to (no of reviews,no of words (max_doc_len),embed _size) in our case.**\n",
    "\n",
    "**TO CLARIFY THIS POINT MUST WATCH NG VIDEOS AND ESECIALLY VO SENTIMENT WAALI.**\n",
    "\n",
    "\n",
    "\n",
    "3) NOW MOVING ON AB BASICALLY HAMARE PASS LSTM LAYER LAGAKE EK VECTOR AA JAAYEGA (h(t)) last lstm cell ka jo ab kisi bhi dense layer me feed kar sakte hai normal NN ki tarah jaaha hum ek vector NN me feed karte hai.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COMPILING AND FITTING THE MODEL AS ALWAYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t3pWbnYPGqBy"
   },
   "outputs": [],
   "source": [
    "# compiling the model. parameters can be tuned as always.\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=1e-3),loss='binary_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "mISkH_MMGqOg",
    "outputId": "8e642c26-2972-40e3-de79-ef5014323c2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 12, 100)           5000      \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, 64)                42496     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 49,642\n",
      "Trainable params: 49,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary() # summary of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dhyaan se har layer ka output and input vagerah ka shape dekhna.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FflnL0O0F8im"
   },
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "epochs=5\n",
    "\n",
    "Y=[0,1,1] # just random kuch matlab nai hai bas model fit karke dikhaana hai isliye kuch le liya\n",
    "Y=to_categorical(Y)\n",
    "x_train,x_test,y_train,y_test=train_test_split(np.array(pad_corp),Y,test_size=0.20,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "aLaVeEG2OfKr",
    "outputId": "1fad6496-35bb-4ef2-d883-96836b06b8ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2 samples, validate on 1 samples\n",
      "Epoch 1/5\n",
      "2/2 [==============================] - 2s 820ms/step - loss: 0.6924 - acc: 0.5000 - val_loss: 0.7034 - val_acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6817 - acc: 1.0000 - val_loss: 0.7168 - val_acc: 0.0000e+00\n",
      "Epoch 3/5\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6706 - acc: 1.0000 - val_loss: 0.7294 - val_acc: 0.0000e+00\n",
      "Epoch 4/5\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6593 - acc: 1.0000 - val_loss: 0.7425 - val_acc: 0.0000e+00\n",
      "Epoch 5/5\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6455 - acc: 1.0000 - val_loss: 0.7593 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7c29320390>"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,batch_size=batch_size,epochs=epochs,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6hhxtIaW1n8Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BONUS::\n",
    "\n",
    "just ek aur tarika hai model banane ka using keras functional API and not the Sequential wala tarika."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp=Input(shape=(samples,max_doc_len)) # None then shape is inferred from input.\n",
    "emb=Embedding(input_dim=vocab_size,output_dim=embed_size,input_length=max_doc_len)(inp)\n",
    "# input_length hata bhi sakte yaha. read docs carefully\n",
    "lstm=CuDNNLSTM(32)(emb)\n",
    "# ab normal dense layers\n",
    "out=Dense(16,activation='relu')\n",
    "out=Dense(2,activaftion='sigmoid') # final layer binary classification.\n",
    "\n",
    "# bas iske baad fir voi compile karke normal fit lagake train vagerah kar sakte hai."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM Keras Practice for sentiemnt analysis.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
